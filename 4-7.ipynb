{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第四章：基于概率论的分类方法：朴素贝叶斯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。本章首先介绍贝叶斯分类算法的基础——贝叶斯定理。最后，我们通过实例来讨论贝叶斯分类的中最简单的一种: 朴素贝叶斯分类。\n",
    "\n",
    "所谓朴素贝叶斯，就是认为属性都是互相独立的，并且权重都是一样的。\n",
    "\n",
    "朴素贝叶斯算法特点\n",
    "- 优点：数据较少情况仍然有效，可以处理多类别问题\n",
    "- 缺点：对输入数据的准备方式比较敏感\n",
    "- 适用数据类型：标称型数据\n",
    "\n",
    "开发流程\n",
    "- 收集数据: 可以使用任何方法\n",
    "- 准备数据: 从文本中构建词向量\n",
    "- 分析数据: 检查词条确保解析的正确性\n",
    "- 训练算法: 从词向量计算概率\n",
    "- 测试算法: 根据现实情况修改分类器\n",
    "- 使用算法: 对社区留言板言论进行分类\n",
    "\n",
    "解决下溢出就用取ln的方式把乘法转化成加法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.屏蔽侮辱性词汇言论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "贝叶斯公式\n",
    "p(xy)=p(x|y)p(y)=p(y|x)p(x)\n",
    "p(x|y)=p(y|x)p(x)/p(y)\n",
    "\"\"\"\n",
    "\n",
    "# ------项目案例1: 屏蔽社区留言板的侮辱性言论------\n",
    "\n",
    "\n",
    "def load_data_set():\n",
    "    \"\"\"\n",
    "    创建数据集,都是假的 fake data set \n",
    "    :return: 单词列表posting_list, 所属类别class_vec\n",
    "    \"\"\"\n",
    "    posting_list = [\n",
    "        ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "        ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "        ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "        ['stop', 'posting', 'stupid', 'worthless', 'gar e'],\n",
    "        ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "        ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    class_vec = [0, 1, 0, 1, 0, 1]  # 1 is 侮辱性的文字, 0 is not\n",
    "    return posting_list, class_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_list(data_set):\n",
    "    \"\"\"\n",
    "    获取所有单词的集合\n",
    "    :param data_set: 数据集\n",
    "    :return: 所有单词的集合(即不含重复元素的单词列表)\n",
    "    \"\"\"\n",
    "    vocab_set = set()  # create empty set\n",
    "    for item in data_set:\n",
    "        # | 求两个集合的并集\n",
    "        vocab_set = vocab_set | set(item)\n",
    "    return list(vocab_set)\n",
    "\n",
    "\n",
    "def set_of_words2vec(vocab_list, input_set):\n",
    "    \"\"\"\n",
    "    遍历查看该单词是否出现，出现该单词则将该单词置1\n",
    "    :param vocab_list: 所有单词集合列表\n",
    "    :param input_set: 输入数据集\n",
    "    :return: 匹配列表[0,1,0,1...]，其中 1与0 表示词汇表中的单词是否出现在输入的数据集中\n",
    "    \"\"\"\n",
    "    # 创建一个和词汇表等长的向量，并将其元素都设置为0\n",
    "    result = [0] * len(vocab_list)\n",
    "    # 遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1\n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            result[vocab_list.index(word)] = 1\n",
    "        else:\n",
    "            # 这个后面应该注释掉，因为对你没什么用，这只是为了辅助调试的\n",
    "            # print('the word: {} is not in my vocabulary'.format(word))\n",
    "            pass\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_naive_bayes(train_mat, train_category):\n",
    "    \"\"\"\n",
    "    朴素贝叶斯分类原版\n",
    "    :param train_mat:  type is ndarray\n",
    "                    总的输入文本，大致是 [[0,1,0,1], [], []]\n",
    "    :param train_category: 文件对应的类别分类， [0, 1, 0],\n",
    "                            列表的长度应该等于上面那个输入文本的长度\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    train_doc_num = len(train_mat)# 矩阵的每一行代表一个文件，矩阵的一行的列数代表每个文件的单词数\n",
    "    words_num = len(train_mat[0])\n",
    "    # 因为侮辱性的被标记为了1， 所以只要把他们相加就可以得到侮辱性的有多少\n",
    "    # 侮辱性文件的出现概率，即train_category中所有的1的个数，\n",
    "    # 代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率\n",
    "    pos_abusive = np.sum(train_category) / train_doc_num\n",
    "    # 单词出现的次数\n",
    "    # 原版\n",
    "    p0num = np.zeros(words_num)\n",
    "    p1num = np.zeros(words_num)\n",
    "\n",
    "    # 整个数据集单词出现的次数（原来是0，后面改成2了）\n",
    "    p0num_all = 0\n",
    "    p1num_all = 0\n",
    "\n",
    "    for i in range(train_doc_num):\n",
    "        # 遍历文件，两个分类，算出的是，侮辱性词汇出现次数，侮辱性文件单词总数；非侮辱性词汇出现次数，非侮辱性文件单词总数\n",
    "        if train_category[i] == 1:\n",
    "            p1num += train_mat[i]\n",
    "            p1num_all += np.sum(train_mat[i])\n",
    "        else:\n",
    "            p0num += train_mat[i]\n",
    "            p0num_all += np.sum(train_mat[i])\n",
    "    # 后面需要改成改成取 log 函数\n",
    "    p1vec = p1num / p1num_all\n",
    "    p0vec = p0num / p0num_all\n",
    "    return p0vec, p1vec, pos_abusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOPosts,listClasses = load_data_set()\n",
    "myVocabList = create_vocab_list(listOPosts)\n",
    "trainMat = []\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(set_of_words2vec(myVocabList, postinDoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0V, p1V, pAb = _train_naive_bayes(trainMat, listClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04166667, 0.04166667, 0.04166667, 0.        , 0.04166667,\n",
       "       0.04166667, 0.        , 0.04166667, 0.        , 0.04166667,\n",
       "       0.04166667, 0.04166667, 0.04166667, 0.        , 0.04166667,\n",
       "       0.04166667, 0.04166667, 0.04166667, 0.04166667, 0.04166667,\n",
       "       0.04166667, 0.125     , 0.        , 0.04166667, 0.        ,\n",
       "       0.        , 0.        , 0.04166667, 0.        , 0.        ,\n",
       "       0.        , 0.08333333])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.15789474, 0.        ,\n",
       "       0.        , 0.05263158, 0.        , 0.05263158, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.05263158, 0.        ,\n",
       "       0.        , 0.        , 0.05263158, 0.05263158, 0.        ,\n",
       "       0.        , 0.        , 0.05263158, 0.        , 0.05263158,\n",
       "       0.05263158, 0.10526316, 0.10526316, 0.05263158, 0.05263158,\n",
       "       0.05263158, 0.05263158])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'mr',\n",
       " 'please',\n",
       " 'stupid',\n",
       " 'cute',\n",
       " 'help',\n",
       " 'buying',\n",
       " 'problems',\n",
       " 'maybe',\n",
       " 'flea',\n",
       " 'how',\n",
       " 'steak',\n",
       " 'has',\n",
       " 'posting',\n",
       " 'is',\n",
       " 'licks',\n",
       " 'love',\n",
       " 'to',\n",
       " 'stop',\n",
       " 'dalmation',\n",
       " 'so',\n",
       " 'my',\n",
       " 'gar e',\n",
       " 'ate',\n",
       " 'food',\n",
       " 'quit',\n",
       " 'worthless',\n",
       " 'dog',\n",
       " 'not',\n",
       " 'park',\n",
       " 'take',\n",
       " 'him']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myVocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    \"\"\"\n",
    "    训练数据优化版本\n",
    "    :param trainMatrix: 文件单词矩阵\n",
    "    :param trainCategory: 文件对应的类别\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 总文件数\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    # 总单词数\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # 侮辱性文件的出现概率\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    # 构造单词出现次数列表\n",
    "    # p0Num 正常的统计\n",
    "    # p1Num 侮辱的统计 \n",
    "    # 避免单词列表中的任何一个单词为0，而导致最后的乘积为0，所以将每个单词的出现次数初始化为 1\n",
    "    p0Num = ones(numWords)#[0,0......]->[1,1,1,1,1.....]\n",
    "    p1Num = ones(numWords)\n",
    "\n",
    "    # 整个数据集单词出现总数，2.0根据样本/实际调查结果调整分母的值（2主要是避免分母为0，当然值可以调整）\n",
    "    # p0Denom 正常的统计\n",
    "    # p1Denom 侮辱的统计\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            # 累加辱骂词的频次\n",
    "            p1Num += trainMatrix[i]\n",
    "            # 对每篇文章的辱骂的频次 进行统计汇总\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    # 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表\n",
    "    p1Vect = log(p1Num / p1Denom)\n",
    "    # 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表\n",
    "    p0Vect = log(p0Num / p0Denom)\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(train_mat, train_category):\n",
    "    \"\"\"\n",
    "    朴素贝叶斯分类修正版，　注意和原来的对比，为什么这么做可以查看书\n",
    "    :param train_mat:  type is ndarray\n",
    "                    总的输入文本，大致是 [[0,1,0,1], [], []]\n",
    "    :param train_category: 文件对应的类别分类， [0, 1, 0],\n",
    "                            列表的长度应该等于上面那个输入文本的长度\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    train_doc_num = len(train_mat)\n",
    "    words_num = len(train_mat[0])\n",
    "    # 因为侮辱性的被标记为了1， 所以只要把他们相加就可以得到侮辱性的有多少\n",
    "    # 侮辱性文件的出现概率，即train_category中所有的1的个数，\n",
    "    # 代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率\n",
    "    pos_abusive = np.sum(train_category) / train_doc_num\n",
    "    # 单词出现的次数\n",
    "    # 原版，变成ones是修改版，这是为了防止数字过小溢出\n",
    "    # p0num = np.zeros(words_num)\n",
    "    # p1num = np.zeros(words_num)\n",
    "    p0num = np.ones(words_num)\n",
    "    p1num = np.ones(words_num)\n",
    "    # 整个数据集单词出现的次数（原来是0，后面改成2了）\n",
    "    p0num_all = 2.0\n",
    "    p1num_all = 2.0\n",
    "\n",
    "    for i in range(train_doc_num):\n",
    "        # 遍历所有的文件，如果是侮辱性文件，就计算此侮辱性文件中出现的侮辱性单词的个数\n",
    "        if train_category[i] == 1:\n",
    "            p1num += train_mat[i]\n",
    "            p1num_all += np.sum(train_mat[i])\n",
    "        else:\n",
    "            p0num += train_mat[i]\n",
    "            p0num_all += np.sum(train_mat[i])\n",
    "    # 后面改成取 log 函数\n",
    "    p1vec = np.log(p1num / p1num_all)\n",
    "    p0vec = np.log(p0num / p0num_all)\n",
    "    return p0vec, p1vec, pos_abusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    \"\"\"\n",
    "    使用算法：\n",
    "        # 将乘法转换为加法\n",
    "        乘法：P(C|F1F2...Fn) = P(F1F·2...Fn|C)P(C)/P(F1F2...Fn)\n",
    "        加法：P(F1|C)*P(F2|C)....P(Fn|C)P(C) -> log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "    :param vec2Classify: 待测数据[0,1,1,1,1...]，即要分类的向量\n",
    "    :param p0Vec: 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表\n",
    "    :param p1Vec: 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表\n",
    "    :param pClass1: 类别1，侮辱性文件的出现概率\n",
    "    :return: 类别1 or 0\n",
    "    \n",
    "    真正计算p1和p0，两边都要除以  p（w），其中w就是待分类的向量。用对数方法  就是要减去   -ln【p（w）】，两个都减去同样的值，这里就省去了。要是\n",
    "    计算的化，要做一个整体词频概率向量表。然后乘一下，用sum求和就行了。\n",
    "    \"\"\"\n",
    "    # 计算公式  log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "    # 使用 NumPy 数组来计算两个向量相乘的结果，这里的相乘是指对应元素相乘，即先将两个向量中的第一个元素相乘，然后将第2个元素相乘，以此类推。\n",
    "    # 我的理解是：这里的 vec2Classify * p1Vec 的意思就是将每个词与其对应的概率相关联起来\n",
    "    # 可以理解为 1.单词在词汇表中的条件下，文件是good 类别的概率 也可以理解为 2.在整个空间下，文件既在词汇表中又是good类别的概率\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)  # \n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_naive_bayes(vec2classify, p0vec, p1vec, p_class1):\n",
    "    \"\"\"\n",
    "    使用算法：\n",
    "        # 将乘法转换为加法\n",
    "        乘法：P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C)/P(F1F2...Fn)\n",
    "        加法：P(F1|C)*P(F2|C)....P(Fn|C)P(C) -> log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "    :param vec2classify: 待测数据[0,1,1,1,1...]，即要分类的向量\n",
    "    :param p0vec: 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表\n",
    "    :param p1vec: 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表\n",
    "    :param p_class1: 类别1，侮辱性文件的出现概率\n",
    "    :return: 类别1 or 0\n",
    "    \"\"\"\n",
    "    # 计算公式  log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "    # 使用 NumPy 数组来计算两个向量相乘的结果，这里的相乘是指对应元素相乘，即先将两个向量中的第一个元素相乘，然后将第2个元素相乘，以此类推。\n",
    "    # 我的理解是：这里的 vec2Classify * p1Vec 的意思就是将每个词与其对应的概率相关联起来\n",
    "    # 可以理解为 1.单词在词汇表中的条件下，文件是good 类别的概率 也可以理解为 2.在整个空间下，文件既在词汇表中又是good类别的概率\n",
    "    p1 = np.sum(vec2classify * p1vec) + np.log(p_class1)\n",
    "    p0 = np.sum(vec2classify * p0vec) + np.log(1 - p_class1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_naive_bayes():\n",
    "    \"\"\"\n",
    "    测试朴素贝叶斯算法\n",
    "    :return: no return \n",
    "    \"\"\"\n",
    "    # 1. 加载数据集\n",
    "    list_post, list_classes = load_data_set()\n",
    "    # 2. 创建单词集合\n",
    "    vocab_list = create_vocab_list(list_post)\n",
    "\n",
    "    # 3. 计算单词是否出现并创建数据矩阵\n",
    "    train_mat = []\n",
    "    for post_in in list_post:\n",
    "        train_mat.append(\n",
    "            # 返回m*len(vocab_list)的矩阵， 记录的都是0，1信息\n",
    "            # 其实就是那个东西的句子向量（就是data_set里面每一行,也不算句子吧)\n",
    "            set_of_words2vec(vocab_list, post_in)\n",
    "        )\n",
    "    # 4. 训练数据\n",
    "    p0v, p1v, p_abusive = train_naive_bayes(np.array(train_mat), np.array(list_classes))\n",
    "    # 5. 测试数据\n",
    "    test_one = ['love', 'my', 'dalmation']\n",
    "    test_one_doc = np.array(set_of_words2vec(vocab_list, test_one))\n",
    "    print('the result is: {}'.format(classify_naive_bayes(test_one_doc, p0v, p1v, p_abusive)))\n",
    "    test_two = ['stupid', 'garbage']\n",
    "    test_two_doc = np.array(set_of_words2vec(vocab_list, test_two))\n",
    "    print('the result is: {}'.format(classify_naive_bayes(test_two_doc, p0v, p1v, p_abusive)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the result is: 0\n",
      "the result is: 1\n"
     ]
    }
   ],
   "source": [
    "testing_naive_bayes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 过滤垃圾邮件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_words2vec(vocab_list, input_set):\n",
    "    # 注意和原来的做对比，词袋模型\n",
    "    result = [0] * len(vocab_list)\n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            result[vocab_list.index(word)] += 1\n",
    "        else:\n",
    "            print('the word: {} is not in my vocabulary'.format(word))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_parse(big_str):\n",
    "    \"\"\"\n",
    "    这里就是做词划分\n",
    "    :param big_str: 某个被拼接后的字符串\n",
    "    :return: 全部是小写的word列表，去掉少于 2 个字符的字符串\n",
    "    \"\"\"\n",
    "    import re\n",
    "    # 其实这里比较推荐用　\\W+ 代替 \\W*，\n",
    "    # 因为 \\W*会match empty patten，在py3.5+之后就会出现什么问题，推荐自己修改尝试一下，可能就会re.split理解更深了\n",
    "    token_list = re.split(r'\\W+', big_str)\n",
    "    if len(token_list) == 0:\n",
    "        print(token_list)\n",
    "    return [tok.lower() for tok in token_list if len(tok) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是从数据中，先进行数据处理，然后随机抽取一些训练集，然后从中抽取一些测试集。然后训练，测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spam_test():\n",
    "    \"\"\"\n",
    "    对贝叶斯垃圾邮件分类器进行自动化处理。\n",
    "    :return: nothing\n",
    "    \"\"\"\n",
    "    doc_list = []\n",
    "    class_list = []\n",
    "    full_text = []\n",
    "    for i in range(1, 26):\n",
    "        # 添加垃圾邮件信息\n",
    "        # 这里需要做一个说明，为什么我会使用try except 来做\n",
    "        # 因为我们其中有几个文件的编码格式是 windows 1252　（spam: 17.txt, ham: 6.txt...)\n",
    "        # 这里其实还可以 :\n",
    "        # import os\n",
    "        # 然后检查 os.system(' file {}.txt'.format(i))，看一下返回的是什么\n",
    "        # 如果正常能读返回的都是：　ASCII text\n",
    "        # 对于except需要处理的都是返回： Non-ISO extended-ASCII text, with very long lines\n",
    "        try:\n",
    "            words = text_parse(open('spam/{}.txt'.format(i)).read())\n",
    "        except:\n",
    "            words = text_parse(open('spam/{}.txt'.format(i), encoding='Windows 1252').read())\n",
    "        doc_list.append(words)\n",
    "        full_text.extend(words)\n",
    "        class_list.append(1)\n",
    "        try:\n",
    "            # 添加非垃圾邮件\n",
    "            words = text_parse(open('ham/{}.txt'.format(i)).read())\n",
    "        except:\n",
    "            words = text_parse(open('ham/{}.txt'.format(i), encoding='Windows 1252').read())\n",
    "        doc_list.append(words)\n",
    "        full_text.extend(words)\n",
    "        class_list.append(0)\n",
    "    # 创建词汇表\n",
    "    vocab_list = create_vocab_list(doc_list)\n",
    "    \n",
    "    import random\n",
    "    # 生成随机取10个数, 为了避免警告将每个数都转换为整型\n",
    "    test_set = [int(num) for num in random.sample(range(50), 10)]\n",
    "    # 并在原来的training_set中去掉这10个数\n",
    "    training_set = list(set(range(50)) - set(test_set))\n",
    "    \n",
    "    training_mat = []\n",
    "    training_class = []\n",
    "    for doc_index in training_set:\n",
    "        training_mat.append(set_of_words2vec(vocab_list, doc_list[doc_index]))\n",
    "        training_class.append(class_list[doc_index])\n",
    "    p0v, p1v, p_spam = train_naive_bayes(\n",
    "        np.array(training_mat),\n",
    "        np.array(training_class)\n",
    "    )\n",
    "\n",
    "    print(p_spam)\n",
    "    \n",
    "    # 开始测试\n",
    "    error_count = 0\n",
    "    for doc_index in test_set:\n",
    "        word_vec = set_of_words2vec(vocab_list, doc_list[doc_index])\n",
    "        if classify_naive_bayes(\n",
    "            np.array(word_vec),\n",
    "            p0v,\n",
    "            p1v,\n",
    "            p_spam\n",
    "        ) != class_list[doc_index]:\n",
    "            error_count += 1\n",
    "    print('the error rate is {}'.format(\n",
    "        error_count / len(test_set)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "the error rate is 0.0\n"
     ]
    }
   ],
   "source": [
    "spam_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取广告中区域倾向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_most_freq(vocab_list, full_text):\n",
    "    # RSS源分类器及高频词去除函数\n",
    "    from operator import itemgetter\n",
    "    freq_dict = {}\n",
    "    for token in vocab_list:\n",
    "        freq_dict[token] = full_text.count(token)\n",
    "    sorted_freq = sorted(freq_dict.items(), key=itemgetter(1), reverse=True)\n",
    "    return sorted_freq[0:30]\n",
    "\n",
    "\n",
    "def local_words(feed1, feed0):\n",
    "    # import feedparser # 其实呢，这一行没用到，最好删了\n",
    "    # 下面操作和上面那个 spam_test函数基本一样，理解了一个，两个都ok\n",
    "    doc_list = []\n",
    "    class_list = []\n",
    "    full_text = []\n",
    "    # 找出两个中最小的一个\n",
    "    min_len = min(len(feed0), len(feed1))\n",
    "    for i in range(min_len):\n",
    "        # 类别　１\n",
    "        word_list = text_parse(feed1['entries'][i]['summary'])\n",
    "        doc_list.append(word_list)\n",
    "        full_text.extend(word_list)\n",
    "        class_list.append(1)\n",
    "        # 类别　０\n",
    "        word_list = text_parse(feed0['entries'][i]['summary'])\n",
    "        doc_list.append(word_list)\n",
    "        full_text.extend(word_list)\n",
    "        class_list.append(0)\n",
    "    vocab_list = create_vocab_list(doc_list)\n",
    "    # 去掉高频词\n",
    "    top30words = calc_most_freq(vocab_list, full_text)\n",
    "    for pair in top30words:\n",
    "        if pair[0] in vocab_list:\n",
    "            vocab_list.remove(pair[0])\n",
    "    # 获取训练数据和测试数据\n",
    "    \n",
    "    import random\n",
    "    # 生成随机取10个数, 为了避免警告将每个数都转换为整型\n",
    "    test_set = [int(num) for num in random.sample(range(2 * min_len), 20)]\n",
    "    # 并在原来的training_set中去掉这10个数\n",
    "    training_set = list(set(range(2 * min_len)) - set(test_set))\n",
    "    \n",
    "    # 把这些训练集和测试集变成向量的形式\n",
    "    training_mat = []\n",
    "    training_class = []\n",
    "    for doc_index in training_set:\n",
    "        training_mat.append(bag_words2vec(vocab_list, doc_list[doc_index]))\n",
    "        training_class.append(class_list[doc_index])\n",
    "    p0v, p1v, p_spam = train_naive_bayes(\n",
    "        np.array(training_mat),\n",
    "        np.array(training_class)\n",
    "    )\n",
    "    error_count = 0\n",
    "    for doc_index in test_set:\n",
    "        word_vec = bag_words2vec(vocab_list, doc_list[doc_index])\n",
    "        if classify_naive_bayes(\n",
    "            np.array(word_vec),\n",
    "            p0v,\n",
    "            p1v,\n",
    "            p_spam\n",
    "        ) != class_list[doc_index]:\n",
    "            error_count += 1\n",
    "    print(\"the error rate is {}\".format(error_count / len(test_set)))\n",
    "    return vocab_list, p0v, p1v\n",
    "\n",
    "\n",
    "def test_rss():\n",
    "    import feedparser\n",
    "    ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "    sf = feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')\n",
    "    vocab_list, p_sf, p_nf = local_words(ny, sf)\n",
    "    # 返回值都没用上，可以用_, _, _代替\n",
    "\n",
    "\n",
    "def get_top_words():\n",
    "    import feedparser\n",
    "    ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "    sf = feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')\n",
    "    vocab_list, p_sf, p_ny = local_words(ny, sf)\n",
    "    top_ny = []\n",
    "    top_sf = []\n",
    "    for i in range(len(p_sf)):\n",
    "        if p_sf[i] > -6.0:\n",
    "            top_sf.append((vocab_list[i], p_sf[i]))\n",
    "        if p_ny[i] > -6.0:\n",
    "            top_ny.append((vocab_list[i], p_ny[i]))\n",
    "    sorted_sf = sorted(top_sf, key=lambda pair: pair[1], reverse=True)\n",
    "    sorted_ny = sorted(top_ny, key=lambda pair: pair[1], reverse=True)\n",
    "    print('\\n----------- this is SF ---------------\\n')\n",
    "    for item in sorted_sf:\n",
    "        print(item[0])\n",
    "    print('\\n----------- this is NY ---------------\\n')\n",
    "    for item in sorted_ny:\n",
    "        print(item[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'feedparser'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-d7a2d578f916>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_rss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-115-1e138eb7f975>\u001b[0m in \u001b[0;36mtest_rss\u001b[1;34m()\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest_rss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mfeedparser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[0mny\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeedparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http://newyork.craigslist.org/stp/index.rss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0msf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeedparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http://sfbay.craigslist.org/stp/index.rss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'feedparser'"
     ]
    }
   ],
   "source": [
    "test_rss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
